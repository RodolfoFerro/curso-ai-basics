{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yTa3GJxFP7lg",
        "Hf27UoWKQus-"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodolfoFerro/curso-ai-basics/blob/main/notebooks/session_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ingenier√≠a de Datos ‚õë\n",
        "\n",
        "La ingenier√≠a de datos se refiere a la construcci√≥n de sistemas para habilitar la recopilaci√≥n y el uso de datos. Estos datos generalmente se utilizan para permitir an√°lisis posteriores y ciencia de datos; lo que a menudo implica aprendizaje autom√°tico. Hacer que los datos sean utilizables generalmente implica un uso sustancial de c√≥mputo y almacenamiento, as√≠ como procesamiento de datos.\n",
        "\n",
        "- Extracci√≥n de datos\n",
        "- Imputaci√≥n de datos\n",
        "- Estandarizaci√≥n/Transformaci√≥n de datos\n",
        "- Encoding de Datos Categ√≥ricos\n",
        "- Filtros/Reducci√≥n de Dimensiones"
      ],
      "metadata": {
        "id": "9-ISa74OIV7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracci√≥n de datos üìÑ\n",
        "Hay muchas maneras de extraer los datos, las principales son:\n",
        "- APIs (SQL, databases, etc... cuentan)\n",
        "- Web Scrapping\n",
        "- Formularios literalmente"
      ],
      "metadata": {
        "id": "V3E6Mb-tJNfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚õèÔ∏èWeb Scraping (MUCHOS PASOS)\n",
        "Es el proceso de usar bots o t√©cnicas de programaci√≥n para extraer contenido y datos de alg√∫n sitio web üë©‚Äçüíª.\n",
        "\n",
        "En nuestro caso lo usaremos para entrar a la red de la UG üêù y ¬°Sacar los horarios ü§ó!\n",
        "\n",
        "Aqu√≠ el link üåû: http://www.dci.ugto.mx/estudiantes/index.php/mcursos/horarios-licenciatura"
      ],
      "metadata": {
        "id": "yTa3GJxFP7lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos a necesitar dos librer√≠as üïù\n",
        "import requests as req # Library for HTTP requests (allows you to send HTTP requests etremely easily): https://pypi.org/project/requests/\n",
        "from bs4 import BeautifulSoup # Python Library for pulling data out of HTML files (or in this case, web page): https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
      ],
      "metadata": {
        "id": "5v_djfPfJM-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mandemos nuestro primer HTTP request a la p√°gina de la UG ü™∞\n",
        "url = 'http://www.dci.ugto.mx/estudiantes/index.php/mcursos/horarios-licenciatura'\n",
        "res = req.get(url) # req.get(url)"
      ],
      "metadata": {
        "id": "71BBIUrEP9eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Necesitamos navegar por el contenido, para eso es BeautifulSoup ‚ú®!\n",
        "soup = BeautifulSoup(res.content, 'html.parser') # BeautifulSoup(res.content, 'html.parser') # Le pasamos el HTML y le indicamos que es tipo html\n",
        "# Obtengamos todas las materias y met√°moslas a una lista! (Hint: No lo veas en la variable, veelo en la p√°gina con el inspector)\n",
        "all_tables = soup.find_all('table') # Hint 2: Necesitamos la tabla, es decir la etiqueta <table> con: soup.find_all('table')\n",
        "# Podemos ver cu√°ntas tablas hay\n",
        "len(all_tables) # len(all_tables)"
      ],
      "metadata": {
        "id": "2sICcJ5AP_E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ya sabemos qu√© tabla necesitamos, guardemosla\n",
        "scedule_table = all_tables[0] # all_tables[1]\n",
        "# Podemos ver el tipo\n",
        "type(scedule_table) # type(scedule_table)"
      ],
      "metadata": {
        "id": "SV51QncnQAqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos pandas\n",
        "import pandas as pd # For Data Analysis and Manipulation in Python: https://pandas.pydata.org/"
      ],
      "metadata": {
        "id": "9U9kKEtTQFW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Primero que nada, columnas, tenemos que sacar el nombre de las columnas que sean mas amigables\n",
        "columns = ['page_number', 'name', 'group', 'day/place/time1', 'day/place/time2', 'day/place/time3', 'day/place/time4', 'teacher']"
      ],
      "metadata": {
        "id": "1uB0Yf6cQJL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora a guardar la de todas las materias (fooooooorr)\n",
        "schedules = []\n",
        "all_rows = scedule_table.find_all('tr')\n",
        "for row in  all_rows[1:]: # row in all_rows[1:]: IGNORA LA PRIMERA\n",
        "    tds = row.find_all('td') # row.find_all('td')\n",
        "    # Otro foooor?\n",
        "    ssubject = {}\n",
        "    for index, column in enumerate(columns):\n",
        "        ssubject[column] = tds[index].string # {column: tds[index]}\n",
        "\n",
        "    # Agregarlas a nuestra lista de horarios\n",
        "    schedules.append(ssubject) # scedules.append(ssubject)\n",
        "\n",
        "# print(schedules) # Review the first one and the last one"
      ],
      "metadata": {
        "id": "XcSsRXqYQNpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LISTO, podemos crear nuestro dataframe # Explica lo que es un DataFrame\n",
        "raw_schedules_df = pd.DataFrame(schedules, columns = columns) # pd.DataFrame(schedules, columns = columns)\n",
        "# Por fin nuestro Data Frame esta aqui... o no?\n",
        "raw_schedules_df # raw_schedules_df"
      ],
      "metadata": {
        "id": "WrARLrLPQX5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üë©‚Äçüíª APIs\n",
        "API es el acr√≥nimo de Interfaz de Programaci√≥n de Aplicaciones. En el contexto de las API, la palabra Aplicaci√≥n se refiere a cualquier software con una funci√≥n distinta. La Interfaz puede considerarse como un contrato de servicio entre dos aplicaciones. Este contrato define c√≥mo las dos se comunican entre s√≠ utilizando solicitudes y respuestas."
      ],
      "metadata": {
        "id": "Hf27UoWKQus-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para obtener datos de una base de datos necesitas el URL de la informaci√≥n\n",
        "url = \"https://jsonplaceholder.typicode.com/users\""
      ],
      "metadata": {
        "id": "YjvGIQ1NRcb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para cargar los datos es con la libreria requests\n",
        "import requests as req # Library for HTTP requests (allows you to send HTTP requests etremely easily): https://pypi.org/project/requests/"
      ],
      "metadata": {
        "id": "VTZ0gR6VRnXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos la respuesta\n",
        "res = req.get(url)\n",
        "res.status_code # ¬øJal√≥?"
      ],
      "metadata": {
        "id": "fu_lKvQjRrfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = res.json()\n",
        "data[:3] # YA ES UN PYTHON DICTIONARY"
      ],
      "metadata": {
        "id": "TISBDHYyR-JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¬°Y LISTO! As√≠ podemos trabajar ya con los datos... Ahora ¬øqu√© sigue?"
      ],
      "metadata": {
        "id": "NMghgXfASPJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imputaci√≥n de datos üéá\n",
        "La imputaci√≥n de datos se refiere a t√©cnicas usadas para reemplazar valores faltantes en un conjunto de datos."
      ],
      "metadata": {
        "id": "GQY9ukM7SUuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "pnFWb63kSCSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de DataFrame con valores faltantes\n",
        "df = pd.DataFrame({'A': [1, 2, None, 4],\n",
        "                   'B': [5, None, None, 8],\n",
        "                   'C': [10, 11, 12, 13]})\n",
        "df"
      ],
      "metadata": {
        "id": "1OtCPUbqS5Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se puede rellenar con 0's\n",
        "df.fillna(0)"
      ],
      "metadata": {
        "id": "c8hh775jTC6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Imputaci√≥n con la media\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "new_data = imputer.fit_transform(df)\n",
        "df_imputed = pd.DataFrame(new_data, columns=df.columns)\n",
        "df_imputed # Nota que transform√≥ todas a np"
      ],
      "metadata": {
        "id": "4yPGxyJXS7zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estandarizaci√≥n / Normalizaci√≥n de datos\n",
        "La estandarizaci√≥n es el proceso de implementar y desarrollar est√°ndares t√©cnicos basados en el consenso de diferentes partes que incluyen empresas, usuarios, grupos de inter√©s, organizaciones de est√°ndares y gobiernos."
      ],
      "metadata": {
        "id": "Di5PwQUmTpD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de DataFrame\n",
        "df = pd.DataFrame({'A': [1, 2, 3, 4],\n",
        "                   'B': [5, 6, 7, 8],\n",
        "                   'C': [9, 10, 11, 12]})\n",
        "df"
      ],
      "metadata": {
        "id": "Wqtl8cWLToIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizaci√≥n com√∫n\n",
        "df[\"A\"] / df[\"A\"].max()"
      ],
      "metadata": {
        "id": "sxUNzIsnU2y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df / df.max()"
      ],
      "metadata": {
        "id": "mchPahsfp2mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def division(serie):\n",
        "    return serie / serie.max()"
      ],
      "metadata": {
        "id": "HJi2ZT7PVjzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora aplicando esta funcion en todas las columnas\n",
        "df.apply(division, axis=0) # APPLYYYYYYYYYYYY!"
      ],
      "metadata": {
        "id": "fMpggzpoVFC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Estandarizaci√≥n con sklearn Mean = 0\n",
        "scaler = StandardScaler()\n",
        "df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "df_standardized # El punto es que hay diferentes tipos de normalizaci√≥n num√©rica, esta es solo una de ellas"
      ],
      "metadata": {
        "id": "w3lT4IgKUCZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Encoding de Datos Categ√≥ricos\n",
        "El encoding de datos categ√≥ricos implica convertir variables categ√≥ricas en una forma que pueda ser proporcionada a los modelos de ML."
      ],
      "metadata": {
        "id": "E4H3kWj9Vz_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de DataFrame\n",
        "df = pd.DataFrame({'Color': ['rojo', 'verde', 'azul', 'amarillo', 'rojo', 'rojo']})\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "_URbXfECV6B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### M√©todo de encoding \"One-Hot\": Es muy com√∫n y f√°cil de implementar con Pandas en Python. Este m√©todo crea una nueva columna para cada categor√≠a √∫nica en la columna original, con 1s y 0s indicando la presencia de cada categor√≠a."
      ],
      "metadata": {
        "id": "RrET3Z-lXAi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando One-Hot Encoding\n",
        "df_encoded = pd.get_dummies(df, columns=['Color'])\n",
        "\n",
        "df_encoded"
      ],
      "metadata": {
        "id": "RyGnlFi_V81B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tambi√©n hay versi√≥n sklearn para mayor flexibilidad\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# One-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "encoding_data = encoder.fit_transform(df[['Color']])\n",
        "columns = encoder.get_feature_names_out(['Color'])\n",
        "df_encoded = pd.DataFrame(encoding_data, columns=columns)\n",
        "df_encoded"
      ],
      "metadata": {
        "id": "AWQdLRvhXG9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtros/Reducci√≥n de Dimensiones\n",
        "La reducci√≥n de dimensiones busca disminuir el n√∫mero de variables aleatorias bajo consideraci√≥n."
      ],
      "metadata": {
        "id": "Nr18sgK7YBTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'A': [1, 2, 3, 4],\n",
        "                   'B': [5, 6, 7, 8],\n",
        "                   'C': [9, 10, 11, 12]})\n",
        "df"
      ],
      "metadata": {
        "id": "HapJi0aQYGBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "# Reducci√≥n de dimensiones con PCA\n",
        "pca = PCA(n_components=2)\n",
        "new_data = pca.fit_transform(df)\n",
        "df_reduced = pd.DataFrame(new_data, columns=['Componente_1', 'Componente_2'])\n",
        "df_reduced # EL PCA ES LO MAAAXIMOOOOOOOOOOOOOOOOOOO, pero medio complejo de entender (btw ocupas estandarizar casi siempre antes)"
      ],
      "metadata": {
        "id": "3galLMhYYIb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo MUY idealizado"
      ],
      "metadata": {
        "id": "w8hGtuVVZBfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraction\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "\n",
        "# load dataset into Pandas DataFrame\n",
        "df = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])\n",
        "df"
      ],
      "metadata": {
        "id": "KSXWs61qZDNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
        "\n",
        "# Separaci√≥n de los \"features\" en un numpy array\n",
        "x = df.loc[:, features].values\n",
        "\n",
        "# Tambi√©n los targets\n",
        "y = df.loc[:,['target']].values"
      ],
      "metadata": {
        "id": "DdIc7_RXZHjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estandarizando los datos\n",
        "x = StandardScaler().fit_transform(x)"
      ],
      "metadata": {
        "id": "NN-ZFKXFZgwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "\n",
        "principal_components = pca.fit_transform(x)\n",
        "\n",
        "principal_df = pd.DataFrame(data = principal_components\n",
        "             , columns = ['principal component 1', 'principal component 2'])"
      ],
      "metadata": {
        "id": "HUQOAkSDZUWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "principal_df.head()"
      ],
      "metadata": {
        "id": "Z7ZQ3B5pZRiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.concat([principal_df, df[['target']]], axis = 1)\n",
        "final_df"
      ],
      "metadata": {
        "id": "r_9iiM4-Z3Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VISUALIZACION\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "52bjxU8xaHIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "\n",
        "targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
        "colors = ['r', 'g', 'b']\n",
        "for target, color in zip(targets,colors):\n",
        "    indicesToKeep = final_df['target'] == target\n",
        "    ax.scatter(final_df.loc[indicesToKeep, 'principal component 1']\n",
        "               , final_df.loc[indicesToKeep, 'principal component 2']\n",
        "               , c = color\n",
        "               , s = 50)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "metadata": {
        "id": "f3UfG3b8aBts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo MUY real"
      ],
      "metadata": {
        "id": "-hdTiXzBZEBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract data"
      ],
      "metadata": {
        "id": "rCadCLgUbkzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests as req\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "PfSxQAOscPgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://api.airtable.com/v0/app16UnSRCYxUX2Jm/datos_puercos\"\n",
        "token = \"XXXXX\"\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {token}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "res = req.get(url, headers=headers)\n",
        "res.status_code"
      ],
      "metadata": {
        "id": "h7logAjRZFgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "almost_data = res.json()['records']\n",
        "data = list(map(lambda x: x['fields'], almost_data))\n",
        "df = pd.DataFrame(data)\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "Nba0YOzebNYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df) # eeeeeeeeeeeeeeeh???????????????? no estan todas"
      ],
      "metadata": {
        "id": "vfYhOJ_l-Tld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://api.airtable.com/v0/app16UnSRCYxUX2Jm/datos_puercos\"\n",
        "offset = None\n",
        "data = []\n",
        "while True:\n",
        "    current_url = url\n",
        "    if offset:\n",
        "        current_url += f\"?offset={offset}\"\n",
        "\n",
        "    res = req.get(current_url, headers=headers)\n",
        "    if res.status_code in range(200, 300):\n",
        "        almost_data = res.json()\n",
        "        if \"offset\" in almost_data:\n",
        "            offset = almost_data['offset']\n",
        "        else:\n",
        "            offset = None\n",
        "        data += list(map(lambda x: x['fields'], almost_data['records']))\n",
        "    else:\n",
        "        break\n",
        "\n",
        "    if not offset:\n",
        "        break"
      ],
      "metadata": {
        "id": "MOVZbanr8Otz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "w7xh4TXw9MXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A mi me gusta ordenar las columns y las rows\n",
        "columns = [\"page_number\", \"name\", \"group\", \"day/place/time1\", \"day/place/time2\", \"day/place/time3\", \"day/place/time4\", \"teacher\"]\n",
        "df = df[columns]\n",
        "df['page_number'] = df['page_number'].astype(int)\n",
        "df = df.sort_values(\"page_number\").reset_index(drop = True)\n",
        "df.head(6)"
      ],
      "metadata": {
        "id": "s20dQQu43SBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üßπ**Data cleansing**: Limpiar los datos (buscar errores y corregirlos).\n",
        "Es muy probable (estoy seguro) de que hay algunos errores dentro de la tabla de horarios, muchos de los cuales no se encontraran a menos que experimenten con estos datos un buen rato. Por suerte ‚ú® me tienen a m√≠, que ya jugu√© con estos datos mucho tiempo y les puedo decir algunos de los errores y bugs m√°s importantes que tienen estos datos (ü§û esperemos no encontrarnos con m√°s)."
      ],
      "metadata": {
        "id": "qjaW8wI9bnC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Estandaricemos las celdas en blanco, hay que ponerlas NAN o None\n",
        "df_no_spaces = df.replace(np.nan, None)\n",
        "df_no_spaces = df_no_spaces.replace(r'^\\s*$', None, regex=True)\n",
        "df_no_spaces.head()"
      ],
      "metadata": {
        "id": "j8r9ShlVbmnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ùå Acentos y May√∫sculas\n",
        "Estos datos podr√≠an tener alg√∫n campo mal escrito, tal vez algunas veces alguna materia tiene acento y tal vez en alguna otra no lo tienen, por ello, habr√° que *normalizar* el texto dentro de nuestra base de datos.\n",
        "\n",
        "Una de las formas en c√≥mo podr√≠amos normalizar estas celdas es creando una funci√≥n de normalizaci√≥n y aplicandola a cada fila de nuestro Data Frame, esto se puede hacer gracias a *[Apply](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html)* de pandas.\n",
        "\n",
        "\n",
        "Primer paso para usar apply, hagamos una funci√≥n de normalizaci√≥n con una palabra random. Hay que pensar muy bien en qu√© queremos hacer y c√≥mo lo vamos a hacer:\n",
        "Algunos errores comunes que podr√≠a tener un texto son:\n",
        "1. Estar escritos con may√∫sculas y min√∫sculas.\n",
        "2. Tener muchos espacios antes o despu√©s del texto\n",
        "3. Tener acentos (Malo)."
      ],
      "metadata": {
        "id": "Cluxs1aHc2mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata"
      ],
      "metadata": {
        "id": "JFkk8Q-Gc_fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(word):\n",
        "    if word is None:\n",
        "        return None\n",
        "    upper_word = word.upper() # word.upper()\n",
        "    striped_word = upper_word.strip() # upper_word.strip()\n",
        "    normalized_word = unicodedata.normalize('NFD', striped_word) # unicodedata.normalize('NFD', striped_word)\n",
        "    result_word = ''\n",
        "    for letter in normalized_word:\n",
        "        if unicodedata.category(letter) != 'Mn':\n",
        "            result_word += letter\n",
        "\n",
        "    return result_word"
      ],
      "metadata": {
        "id": "KwRwkGKQczEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos a testear esta funcion\n",
        "word = \"    T√ìPICoS SeLeCToS DE AsTrONoM√ça           \"\n",
        "normalized_word = normalize_text(word)\n",
        "normalized_word"
      ],
      "metadata": {
        "id": "RZxIKeLAdBez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apliquemos esta funci√≥n a cada una de las filas de nuestro Data Frame\n",
        "normalized_df = df_no_spaces.apply(normalize_text) # Oh no\n",
        "normalized_df # Oh no! Qu√© sali√≥ mal? ü§î"
      ],
      "metadata": {
        "id": "K4hQZRAVdC-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qu√© sali√≥ mal? ü§î\n",
        "Esta funci√≥n est√° hecha solamente para normalizar texto, por tanto, si le pasamos una fila o columna de un dataframe, lo convertir√° a string! No queremos eso, tenemos que hacer otra funci√≥n para que aplique esta funci√≥n \"*normalize_text*\" a cada una de nuestras filas (O columnas)."
      ],
      "metadata": {
        "id": "uPhNGBxkdJxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_df_rows(row):\n",
        "    # Ciclemos por cada columna en esta fila\n",
        "    normalized_row = [] # Necesitamos una lista para guardar cada una de nuestras filas\n",
        "    for cell in row:\n",
        "        normalized_row.append(normalize_text(cell)) # normalized_row.append(normalize_text(cell)) # Guardando la normalizacion de cada celda\n",
        "\n",
        "    return normalized_row"
      ],
      "metadata": {
        "id": "olikUlB6dITF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora si, testeemos esta funcion\n",
        "normalized_data = df_no_spaces.apply(normalize_df_rows, axis = 0) # normalized_data = raw_schedules_df.apply(normalize_df_rows, axis = 1)\n",
        "normalized_data"
      ],
      "metadata": {
        "id": "2CMQWT6adMPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.__version__"
      ],
      "metadata": {
        "id": "lvedBgng0EYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_df = df_no_spaces.applymap(normalize_text)\n",
        "normalized_df"
      ],
      "metadata": {
        "id": "5uBKV2Moz-aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. ¬øCampos en Blanco?\n",
        "Si hay campos en blanco (que no sean de d√≠as porque no todos tienen m√°s de uno) entonces hay que corregirlo porque significa que algo est√° mal en nuestro algoritmo de recolecci√≥n de datos (o en la p√°gina de la UG).\n",
        "\n",
        "http://www.dci.ugto.mx/estudiantes/index.php/mcursos/horarios-licenciatura"
      ],
      "metadata": {
        "id": "K2a9X1lH1dZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_rows = normalized_df[(normalized_df[\"day/place/time1\"].isna()) | (normalized_df[\"teacher\"].isna())]\n",
        "invalid_rows"
      ],
      "metadata": {
        "id": "CkNFX4r71iNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ùå 4. Errores en horarios\n",
        "Necesitamos un est√°ndar para cada una de las columnas, en caso de las columnas de nombres de profesores y de materias, realmente no necesitamos hacer nada (porque ya normalizamos). PERO, en el caso de los horarios de cada materia, ah√≠ s√≠ necesitamos un est√°ndar para que podamos operar con estas cosas de manera correcta en el futuro, el que yo eleg√≠ tiene la forma:\n",
        "\n",
        "_d√≠a/hora_inicio-hora_final/lugar_\n",
        "\n",
        "por ejemplo:\n",
        "\n",
        "_LUNES/14-16/F9_\n",
        "\n",
        "Si alguna fila no cumple con este est√°ndar, entonces habr√° que corregirlo."
      ],
      "metadata": {
        "id": "ZKiCyravC4Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hagamos una funci√≥n para detectar cuando una fecha no est√° en el formato correcto (hay que pensar de lo mas shiquito a lo mas grande cuando no sepas hacer algo)\n",
        "def detect_wrong_dates(date, index = False, column_name = False):\n",
        "    # Los requisitos para esta funcion son que date sea una string diferente de nan, por ello.\n",
        "    if date == 'NAN' or date is None:\n",
        "        return\n",
        "    # Splitear la fecha podria ser util\n",
        "    date_split = date.split('/') # date.split('/')\n",
        "\n",
        "    # A partir de esto podr√≠an ocurrir varios errores o bugs\n",
        "    # 1. Que no tenga alg√∫n \"/\"\n",
        "    # 2. Que tenga m√°s de un \"/\"\n",
        "    # 3. Que haya gui√≥n entre los horarios de inicio y finales, es decir, que no est√© en formato \"hora-hora\"\n",
        "\n",
        "    # Corrijamos los primeros dos\n",
        "    if len(date_split) != 3:\n",
        "        print('Slashes Error')\n",
        "        print(f\"Index={index}, #Pag={index + 1}, Columna={column_name[-1]}\")\n",
        "\n",
        "    # Corrijamos la 3\n",
        "    if len(date_split) == 3:\n",
        "        hours = date_split[1] # Porque queremos checar la hora\n",
        "        if hours and len(hours.split('-')) != 2:\n",
        "            print('Hour Error')\n",
        "            print(f\"Index={index}, #Pag={index + 1}, Columna={column_name[-1]}\")\n",
        "\n",
        "\n",
        "# Aqu√≠ suceder√° de nuevo lo que pas√≥ con la normalizaci√≥n, tenemos que crear otra funci√≥n para que trabaje sobre DataFrames\n",
        "def detect_wrong_dates_in_df(df, date_columns):\n",
        "    # Hay una forma de iterar sobre filas mas sencillo\n",
        "    for index, row in df.iterrows():\n",
        "        for column in date_columns:\n",
        "            detect_wrong_dates(row[column], index, column)"
      ],
      "metadata": {
        "id": "zqtR0NziC7bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testeemos esta funci√≥n, primero necesitamos las columnas donde est√°n las fechas\n",
        "date_columns = ['day/place/time1', 'day/place/time2', 'day/place/time3', 'day/place/time4']\n",
        "detect_wrong_dates_in_df(normalized_df, date_columns) # üò± Asumakina, muchos errores, Maldita UG arruin√≥ la UG"
      ],
      "metadata": {
        "id": "NIMZfy9pC5Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AHORA a cargar los datos:"
      ],
      "metadata": {
        "id": "LBND45uqdXpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_data.to_csv(\"datos_chidos.csv\")"
      ],
      "metadata": {
        "id": "1r5Xb6DzdORl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ¬øQu√© sigue?\n",
        "Darle significado a lo que haces, los datos est√°n ah√≠, y se pueden limpiar y se pueden hacer un mont√≥n de cosas, pero si no les das significado y valor, no sirven de nada, tanto los datos, como tu esfuerzo! D: (A menos que hayas aprendido algo entonces s√≠).\n",
        "Ejemplo:\n",
        "https://bubudavid.github.io/dci-hh/"
      ],
      "metadata": {
        "id": "hVkjm_kxdnNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> Contenido creado por por **David (Bubu)** (2023). <br>\n",
        "> **Contacto:** [@bubusaurio_rex](https://www.instagram.com/bubusaurio_rex/)"
      ],
      "metadata": {
        "id": "Gcihit7oOLQb"
      }
    }
  ]
}